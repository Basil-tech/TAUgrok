import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from itertools import product
from torch.utils.tensorboard import SummaryWriter
import string


# 52 letters (upper + lower case)
ENG_LETTERS = list(string.ascii_letters)  

# 24 letters
GRK_LETTERS = ['α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω']

# 21 Hebrew letters without ב
HEB_LETTERS = ['א', 'ג', 'ד', 'ה', 'ו', 'ז', 'ח', 'ט', 'י', 'כ', 'ל', 'מ', 'נ', 'ס', 'ע', 'פ', 'צ', 'ק', 'ר', 'ש', 'ת']  

LETTERS = ENG_LETTERS + GRK_LETTERS + HEB_LETTERS

EOS = "<EOS>"

SPECIAL_CHARS = ['◦', '=', '/', EOS]

SYMBOLS = LETTERS + SPECIAL_CHARS

P = 97

class Tokenizer:

    def __init__(self):
        self.symbol_to_id = {symbol: i for i, symbol in enumerate(SYMBOLS)}
        self.id_to_symbol = {i: symbol for symbol, i in self.symbol_to_id.items()}

    def encode(self, symbols):
        if isinstance(symbols, str):
            symbols = [symbols]
        return [self.symbol_to_id[s] for s in symbols]

    def decode(self, ids):
        if isinstance(ids, int):
            ids = [ids]
        return [self.id_to_symbol[id] for id in ids]
    
    @property
    def vocab_size(self):
        return len(self.symbol_to_id)


# standard transformer pos encoding from "attention is all you need"
class PositionalEncoding(nn.Module):
    def __init__(self, model_dim, max_len=5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, model_dim, 2) * (-np.log(10000.0) / model_dim))
        pe = torch.zeros(max_len, 1, model_dim)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0)]


class TransformerModel(nn.Module):
    # args from A.1.2 in the paper
    def __init__(self, vocab_size, model_dim=128, nhead=4, num_layers=2, device="cpu"):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, model_dim)
        self.pos_encoder = PositionalEncoding(model_dim)
        # Weirdly, the paper mentions a " standard decoder-only transformer Vaswani et al. 
        # (2017) with causal attention masking". This is implemented as a transformer encoder
        # in pytorch, so we use that.
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(model_dim, nhead, dim_feedforward=512, batch_first=True),
            num_layers=num_layers
        )
        self.prediction = nn.Linear(model_dim, vocab_size)
        self.device = device
        self.model_dim = model_dim

        # i am creating a bigger mask than needed, i'll slice it later according to the seq length
        self.register_buffer('mask', torch.tril(torch.ones(vocab_size, vocab_size)))

        

    def forward(self, x):
        _, seq_len = x.size()
        x = self.embedding(x)
        # The sqrt scaling - again from attention is all you need, 
        # suppose to suppress the magnitude generated by the dot 
        # product in the attention mechanism.
        x = self.pos_encoder(x) * np.sqrt(self.model_dim) 
        causal_mask = self.mask[:seq_len, :seq_len].to(self.device)
        # nice trick - the mask is filled with -inf, causing the softmax inside the transformer to be 0.
        causal_mask = causal_mask.masked_fill(causal_mask == 0, float('-inf')).masked_fill(causal_mask == 1, 0.0)
        x = self.transformer_encoder(x, causal_mask)
        return self.prediction(x)

class BinaryDivisionModDataset(Dataset):

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.data = self._generate_data()

    def _generate_data(self):
        # keeping the raw dataset maybe to reproduce
        # the nice operation tables.
        data = []
        pairs = product(range(P), repeat=2)
        for a, b in pairs:
            if b == 0:
                continue
            c = a
            a = (b * c) % P
            a = self.tokenizer.decode(a)[0]
            b = self.tokenizer.decode(b)[0]
            c = self.tokenizer.decode(c)[0]
            data.append([EOS, a, "/", b, "=", c , EOS])
            
        return data
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x = self.tokenizer.encode(self.data[idx])
        y = x.copy()  # Same as x for causal attention
        return torch.tensor(x), torch.tensor(y)

def train(model, train_loader, optimizer, loss_f, device, writer, epoch):
    model.train()
    total_loss = 0

    for batch, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        output = model(x)
        # output is (batch, seq_len, vocab_size), y is (batch, seq_len)
        # in the paper - "calculated loss and accuracy only on the answer part of the equation"
        # answer idx is 5.
        answer_pred = output[:, 5, :]
        answer_targets = y[:, 5]

        loss = loss_f(answer_pred, answer_targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        final_ans = answer_pred.argmax(dim=1)
        acc = (final_ans == answer_targets).sum().item() / y.size(0)

        # maybe writing to much? Not sure, i want to catch every optim step?
        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch)
        writer.add_scalar('Accuracy/train', acc, epoch * len(train_loader) + batch)
    
    return total_loss / len(train_loader)

def validate(model, val_loader, loss_f, device, writer, epoch):
    model.eval()
    total_loss = 0
    total_acc = 0

    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            output = model(x)
            answer_pred = output[:, 5, :]
            answer_targets = y[:, 5]
            loss = loss_f(answer_pred, answer_targets)
            total_loss += loss.item()
            final_ans = answer_pred.argmax(dim=1)
            total_acc += (final_ans == answer_targets).sum().item() / y.size(0)
            
    writer.add_scalar('Loss/val', total_loss / len(val_loader), epoch)
    writer.add_scalar('Accuracy/val', total_acc / len(val_loader), epoch)

    return total_loss / len(val_loader), total_acc / len(val_loader)

def main():

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = Tokenizer()
    vocab_size = len(tokenizer.symbol_to_id)
    
    full_dataset = BinaryDivisionModDataset(tokenizer=tokenizer)
    train_dataset, val_dataset = random_split(full_dataset, [0.5, 0.5])
    
    bs = min(512, len(train_dataset) // 2) # A.1.2 from the paper
    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=bs)
    model = TransformerModel(vocab_size=vocab_size).to(device)

    # A.1.2 from the paper, hopefully got it right
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1, betas=(0.9, 0.98))
    loss = nn.CrossEntropyLoss()
    tb_summary = SummaryWriter()
    
    n_epochs = 10000

    for epoch in range(n_epochs):
        train_loss = train(model, train_loader, optimizer, loss, device, tb_summary, epoch)
        val_loss, val_accuracy = validate(model, val_loader, loss, device, tb_summary, epoch)
        print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')
    
    tb_summary.close()

# if __name__ == "__main__":
main()